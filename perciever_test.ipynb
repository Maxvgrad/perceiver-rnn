{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "from einops import rearrange\n",
    "from glob import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from torch.nn import L1Loss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "from models.perciever import Perceiver\n",
    "from models.perciever_rnn import PerceiverRNN, MLPPredictor\n",
    "from data_prep.nvidia import NvidiaDatasetRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-05-20-12-36-10_e2e_sulaoja_20_30\n",
      "17952\n",
      "torch.Size([17952, 3])\n",
      "torch.Size([68, 264, 3])\n"
     ]
    }
   ],
   "source": [
    "# dataset_path = 'data_stuff/rally-estonia-cropped-antialias'\n",
    "dataset_path = '/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias'\n",
    "dataset_subdirs = glob(dataset_path + '/*')\n",
    "dataset_subdir = dataset_subdirs[0]\n",
    "print(dataset_subdir)\n",
    "image_paths = glob(dataset_subdir + '/front_wide/*.png')\n",
    "image = Image.open(image_paths[0])\n",
    "pixels = list(image.getdata())\n",
    "print(len(pixels))\n",
    "pixel_tens = torch.Tensor(pixels)\n",
    "print(pixel_tens.shape)\n",
    "pixel_tens = pixel_tens.view(68, 264, 3)\n",
    "print(pixel_tens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "batch = pixel_tens.view(1, *pixel_tens.shape)\n",
    "print(batch.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pmodel = Perceiver(\n",
    "    input_channels = 3,          # number of channels for each token of the input\n",
    "    input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n",
    "    num_freq_bands = 6,          # number of freq bands, with original value (2 * K + 1)\n",
    "    max_freq = 10.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
    "    depth = 1,                   # depth of net. The shape of the final attention mechanism will be:\n",
    "                                 #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
    "    num_latents = 256,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "    latent_dim = 512,            # latent dimension\n",
    "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
    "    latent_heads = 8,            # number of heads for latent self attention, 8\n",
    "    cross_dim_head = 64,         # number of dimensions per cross attention head\n",
    "    latent_dim_head = 64,        # number of dimensions per latent self attention head\n",
    "    num_classes = 1,          # output number of classes\n",
    "    attn_dropout = 0.,\n",
    "    ff_dropout = 0.,\n",
    "    weight_tie_layers = False,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
    "    fourier_encode_data = True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
    "    self_per_cross_attn = 2      # number of self attention blocks per cross attention\n",
    ")\n",
    "\n",
    "steering_classifier = MLPPredictor(512, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NvidiaDataset] Using default transform: Compose(\n",
      "    <data_prep.nvidia.Normalize object at 0x2b44f0964fa0>\n",
      ")\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-10-14-13-08-51_e2e_rec_vahi_backwards: length=13442, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-10-11-17-20-12_e2e_rec_backwards: length=68954, filtered=5\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-06-07-14-06-31_e2e_rec_ss6: length=3003, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-05-20-12-51-29_e2e_sulaoja_20_30: length=5393, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-06-10-13-19-22_e2e_ss4_backwards: length=23844, filtered=1\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-10-11-16-06-44_e2e_rec_ss2: length=81250, filtered=1\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-09-24-12-02-32_e2e_rec_ss10_3: length=8142, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-06-10-15-03-16_e2e_ss3_backwards: length=2458, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-05-28-15-17-19_e2e_sulaoja_20_30: length=3218, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-09-30-15-56-59_e2e_ss14_attempt_2: length=66899, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-09-24-13-39-38_e2e_rec_ss11: length=33255, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-10-11-17-10-23_e2e_rec_last_part: length=4986, filtered=3\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-09-24-12-21-20_e2e_rec_ss10_backwards: length=64975, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-05-20-13-59-00_e2e_sulaoja_10_10: length=687, filtered=1\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-09-24-11-19-25_e2e_rec_ss10: length=34760, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-06-10-12-59-59_e2e_ss4: length=25508, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-06-09-15-42-05_e2e_rec_ss3_backwards: length=40694, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-05-28-15-07-56_e2e_sulaoja_20_30: length=15626, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-09-30-15-20-14_e2e_ss14_backwards: length=52762, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-06-10-14-44-24_e2e_ss3_backwards: length=31838, filtered=0\n",
      "Validation set...\n",
      "[NvidiaDataset] Using default transform: Compose(\n",
      "    <data_prep.nvidia.Normalize object at 0x2b44fc7c3c40>\n",
      ")\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-09-30-15-03-37_e2e_ss14_from_half_way: length=21755, filtered=1\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-05-20-12-36-10_e2e_sulaoja_20_30: length=12025, filtered=1\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-09-24-14-03-45_e2e_rec_ss11_backwards: length=25172, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-05-20-12-43-17_e2e_sulaoja_20_30: length=6809, filtered=0\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-10-20-13-57-51_e2e_rec_neeruti_ss19_22: length=29395, filtered=2\n",
      "/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias/2021-06-09-14-58-11_e2e_rec_ss3: length=43886, filtered=0\n",
      "Dataset ready\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=512\n",
    "NUM_WORKERS=2\n",
    "SEQ_LENGTH=8\n",
    "STRIDE=4\n",
    "\n",
    "# RNN dataloader\n",
    "\n",
    "# dataset_path = Path('./data_stuff/rally-estonia-cropped-antialias')\n",
    "dataset_path = Path('/gpfs/space/projects/rally2023/rally-estonia-cropped-antialias')\n",
    "random.seed(42)\n",
    "data_dirs = os.listdir(dataset_path)\n",
    "random.shuffle(data_dirs)\n",
    "split_index1 = int(0.4 * len(data_dirs))\n",
    "split_index2 = int(0.5 * len(data_dirs))\n",
    "\n",
    "train_paths = [dataset_path / dir_name for dir_name in data_dirs[:split_index1]]\n",
    "valid_paths = [dataset_path / dir_name for dir_name in data_dirs[split_index1:split_index2]]\n",
    "\n",
    "train_dataset = NvidiaDatasetRNN(train_paths, SEQ_LENGTH, STRIDE)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                   num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                                                   persistent_workers=True, collate_fn=train_dataset.collate_fn)\n",
    "print('Validation set...')\n",
    "valid_dataset = NvidiaDatasetRNN(valid_paths, SEQ_LENGTH, STRIDE)\n",
    "print('Dataset ready')\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                                   num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                                                   persistent_workers=False, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/data_prep/nvidia.py\", line 172, in __getitem__\n    target[0, :] = target_values\nValueError: could not broadcast input array from shape (104,) into shape (8,)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# RNN Forward pass\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# shape B, T, C, H, W\u001b[39;00m\n\u001b[1;32m      6\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# shape B, T\u001b[39;00m\n",
      "File \u001b[0;32m~/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/rally-challenge-24/venv/lib/python3.8/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/data_prep/nvidia.py\", line 172, in __getitem__\n    target[0, :] = target_values\nValueError: could not broadcast input array from shape (104,) into shape (8,)\n"
     ]
    }
   ],
   "source": [
    "# RNN Forward pass\n",
    "\n",
    "batch = next(iter(train_loader)) \n",
    "\n",
    "images = batch[0]['image'] # shape B, T, C, H, W\n",
    "targets = batch[1] # shape B, T\n",
    "\n",
    "images = rearrange(images, 'b t c h w -> t b h w c')\n",
    "targets = rearrange(targets, 'b t -> t b')\n",
    "latents = None\n",
    "# for t in range(images.shape[0]):\n",
    "#     t_img_batch = images[t]\n",
    "#     steering, latents = percieverRNN.forward(t_img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/data_prep/nvidia.py\", line 172, in __getitem__\n    target[0, :] = target_values\nValueError: could not broadcast input array from shape (112,) into shape (8,)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tens)):\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(tens[i]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/rally-challenge-24/venv/lib/python3.8/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/gpfs/space/home/okuu/rally-challenge-24/data_prep/nvidia.py\", line 172, in __getitem__\n    target[0, :] = target_values\nValueError: could not broadcast input array from shape (112,) into shape (8,)\n"
     ]
    }
   ],
   "source": [
    "tens = next(iter(train_loader))[0]['image'][0]\n",
    "for i in range(len(tens)):\n",
    "    plt.imshow(tens[i].permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "DECAY = 1e-02\n",
    "SAVE_DIR = Path('trained_models')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "criterion = L1Loss().to(device)\n",
    "model = PerceiverRNN(pmodel, steering_classifier).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=LEARNING_RATE,\n",
    "                              betas=(0.9, 0.999),\n",
    "                              eps=1e-08,\n",
    "                              weight_decay=DECAY,\n",
    "                              amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_whiteness(steering_angles, fps):\n",
    "    current_angles = steering_angles[:-1]\n",
    "    next_angles = steering_angles[1:]\n",
    "    delta_angles = next_angles - current_angles\n",
    "\n",
    "    whiteness = np.sqrt(((delta_angles * fps) ** 2).mean())\n",
    "    return whiteness\n",
    "\n",
    "def calculate_open_loop_metrics(predicted_steering, true_steering, fps):\n",
    "    predicted_degrees = predicted_steering / np.pi * 180\n",
    "    true_degrees = true_steering / np.pi * 180\n",
    "\n",
    "    somewhere_middle = len(predicted_degrees) // 2\n",
    "\n",
    "    errors_signed = predicted_degrees - true_degrees # positive means error to the left\n",
    "    errors = np.abs(errors_signed)\n",
    "    whiteness = calculate_whiteness(predicted_degrees, fps)\n",
    "    expert_whiteness = calculate_whiteness(true_degrees, fps)\n",
    "\n",
    "    return {\n",
    "        'mae': errors.mean(),\n",
    "        'rmse': np.sqrt((errors ** 2).mean()),\n",
    "        'bias': errors_signed.mean(),\n",
    "        'max': errors.max(),\n",
    "        'whiteness': whiteness,\n",
    "        'expert_whiteness': expert_whiteness\n",
    "    }\n",
    "\n",
    "def calculate_metrics(fps, predictions, valid_loader):\n",
    "        frames_df = valid_loader.dataset.frames\n",
    "\n",
    "        true_steering_angles = frames_df.steering_angle.to_numpy()\n",
    "        metrics = calculate_open_loop_metrics(predictions, true_steering_angles, fps=fps)\n",
    "\n",
    "        left_turns = frames_df[\"turn_signal\"] == 0\n",
    "        if left_turns.any():\n",
    "            left_metrics = calculate_open_loop_metrics(predictions[left_turns], true_steering_angles[left_turns], fps=fps)\n",
    "            metrics[\"left_mae\"] = left_metrics[\"mae\"]\n",
    "        else:\n",
    "            metrics[\"left_mae\"] = 0\n",
    "\n",
    "        straight = frames_df[\"turn_signal\"] == 1\n",
    "        if straight.any():\n",
    "            straight_metrics = calculate_open_loop_metrics(predictions[straight], true_steering_angles[straight], fps=fps)\n",
    "            metrics[\"straight_mae\"] = straight_metrics[\"mae\"]\n",
    "        else:\n",
    "            metrics[\"straight_mae\"] = 0\n",
    "\n",
    "        right_turns = frames_df[\"turn_signal\"] == 2\n",
    "        if right_turns.any():\n",
    "            right_metrics = calculate_open_loop_metrics(predictions[right_turns], true_steering_angles[right_turns], fps=fps)\n",
    "            metrics[\"right_mae\"] = right_metrics[\"mae\"]\n",
    "        else:\n",
    "            metrics[\"right_mae\"] = 0\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, progress_bar, epoch):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for i, (data, target_values, condition_mask) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = data['image'].to(device) # (T, B, H, W, C)\n",
    "        target_values = target_values.to(device) # (T, B)\n",
    "        \n",
    "        latents = None\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for t in range(inputs.size(0)):\n",
    "            input_frame = inputs[t]\n",
    "            target_frame = target_values[t]\n",
    "\n",
    "            predictions, latents = model(input_frame, latents)\n",
    "\n",
    "            # Calculate loss for the current time step\n",
    "            loss = criterion(predictions.squeeze(), target_frame)\n",
    "            loss.backward(retain_graph=True if t < inputs.size(0) - 1 else False)\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        pbar_description = f'epoch {epoch+1} | train loss: {(running_loss / (i + 1)):.4f}'\n",
    "        progress_bar.set_description(pbar_description)\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, progress_bar, epoch, train_loss):\n",
    "        epoch_loss = 0.0\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, target_values, condition_mask) in enumerate(loader):\n",
    "                inputs = data['image'].to(device)\n",
    "                target_values = target_values.to(device)\n",
    "                \n",
    "                latents = None\n",
    "                sequence_predictions = []\n",
    "                \n",
    "                for t in range(inputs.size(0)):\n",
    "                    input_frame = inputs[t]\n",
    "                    target_frame = target_values[t]\n",
    "\n",
    "                    predictions, latents = model(input_frame, latents)\n",
    "                    sequence_predictions.append(predictions)\n",
    "\n",
    "                    loss = criterion(predictions.squeeze(), target_frame)\n",
    "                    epoch_loss += loss.item()\n",
    "                \n",
    "                all_predictions.extend(torch.stack(sequence_predictions).cpu().squeeze().numpy())\n",
    "\n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_description(f'epoch {epoch + 1} | train loss: {train_loss:.4f} | valid loss: {(epoch_loss / (i + 1)):.4f}')\n",
    "\n",
    "        total_loss = epoch_loss / len(loader)\n",
    "        result = np.array(all_predictions)\n",
    "        return total_loss, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, n_epoch, patience=10, fps=30, load_checkpoint=False):\n",
    "    CHECKPOINT_FILENAME=\"checkpoint-2.pth.tar\"\n",
    "    \n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "    \n",
    "    if not load_checkpoint:\n",
    "        best_valid_loss = float('inf')\n",
    "        epochs_of_no_improve = 0\n",
    "        epochs_passed = 0\n",
    "        train_loss_list = []\n",
    "        valid_loss_list = []\n",
    "    else:\n",
    "        checkpoint = torch.load(SAVE_DIR / CHECKPOINT_FILENAME)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        epochs_passed = checkpoint['epoch']\n",
    "        best_valid_loss = checkpoint['best_valid_loss']\n",
    "        epochs_of_no_improve = checkpoint['epochs_of_no_improve']\n",
    "        train_loss_list = checkpoint['train_loss_list']\n",
    "        valid_loss_list = checkpoint['valid_loss_list']\n",
    "\n",
    "    if wandb_logging:\n",
    "        wandb.watch(model, criterion)\n",
    "\n",
    "    for epoch in range(n_epoch - epochs_passed):\n",
    "        \n",
    "        progress_bar = tqdm(total=len(train_loader), smoothing=0)\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, progress_bar, epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "        progress_bar.reset(total=len(valid_loader))\n",
    "        valid_loss, predictions = evaluate(model, valid_loader, criterion, progress_bar, epoch, train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            \n",
    "            torch.save(model.state_dict(), SAVE_DIR / f\"best.pt\")\n",
    "            \n",
    "            epochs_of_no_improve = 0\n",
    "            best_loss_marker = '*'\n",
    "        else:\n",
    "            epochs_of_no_improve += 1\n",
    "            best_loss_marker = ''\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch + epoch_passed + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_valid_loss': best_valid_loss,\n",
    "            'epochs_of_no_improve': epochs_of_no_improve,\n",
    "            'train_loss_list': train_loss_list,\n",
    "            'valid_loss_list': valid_loss_list\n",
    "        }, SAVE_DIR / f\"checkpoint-{epoch+epochs_passed}.pth.tar\")\n",
    "            \n",
    "        metrics = calculate_metrics(fps, predictions, valid_loader)\n",
    "        whiteness = metrics['whiteness']\n",
    "        mae = metrics['mae']\n",
    "        left_mae = metrics['left_mae']\n",
    "        straight_mae = metrics['straight_mae']\n",
    "        right_mae = metrics['right_mae']\n",
    "        progress_bar.set_description(f'{best_loss_marker}epoch {epoch + 1}'\n",
    "                                        f' | train loss: {train_loss:.4f}'\n",
    "                                        f' | valid loss: {valid_loss:.4f}'\n",
    "                                        f' | whiteness: {whiteness:.4f}'\n",
    "                                        f' | mae: {mae:.4f}'\n",
    "                                        f' | l_mae: {left_mae:.4f}'\n",
    "                                        f' | s_mae: {straight_mae:.4f}'\n",
    "                                        f' | r_mae: {right_mae:.4f}')\n",
    "\n",
    "        if wandb_logging:\n",
    "            metrics['epoch'] = epoch + 1\n",
    "            metrics['train_loss'] = train_loss\n",
    "            metrics['valid_loss'] = valid_loss\n",
    "            wandb.log(metrics)\n",
    "\n",
    "        if epochs_of_no_improve == patience:\n",
    "            print(f'Early stopping, on epoch: {epoch + 1}.')\n",
    "            break\n",
    "\n",
    "    torch.save(model.state_dict(), SAVE_DIR / \"last.pt\")\n",
    "    pt_models = [f'{SAVE_DIR}/last.pt', f'{SAVE_DIR}/best.pt']\n",
    "\n",
    "    for pt_model_path in pt_models:\n",
    "        model_path = convert_pt_to_onnx(pt_model_path)\n",
    "\n",
    "    return best_valid_loss, train_loss_list, valid_loss_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
